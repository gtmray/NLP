{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Basics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMh2QGHfVPfVgfoTvxkwrSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gtmray/NLP/blob/master/NLP_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZIZDgZfIGJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "14945910-962e-4b79-a98e-ec3e165d7df4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPhL1j28ITbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3375ec76-dd85-443b-e370-199f53330bf8"
      },
      "source": [
        "df1 = pd.read_csv('https://raw.githubusercontent.com/gtmray/imdb/master/imdb_train.csv')\n",
        "df2 = pd.read_csv('https://raw.githubusercontent.com/gtmray/imdb/master/imdb_test.csv')\n",
        "df = df1.append(df2)\n",
        "df_decreased = df.iloc[:50000, :]\n",
        "print(df_decreased.shape)\n",
        "# paragraph = \"My 123 name is Rewan Gautam Rewan. I come from the heart of purbanchal, Itahari. It is biggest city of Sunsari district. I am the person you will not predict even with neural networks.\"\n",
        "# sentence = nltk.sent_tokenize(paragraph)\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLgYz8ONLw-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting sentences to cleaned sentences with \n",
        "# - lowercase\n",
        "# - taking only alphabets\n",
        "# - lemmatization\n",
        "\n",
        "import re\n",
        "stop = stopwords.words('english')\n",
        "cleaned = []\n",
        "sentence = df_decreased['text']\n",
        "for i in sentence:\n",
        "  i = re.sub('[^a-zA-Z]', ' ', i)\n",
        "  i = i.lower()\n",
        "  words = nltk.word_tokenize(i)\n",
        "  temp = [stemmer.stem(word) for word in words if word not in stop]\n",
        "  cleaned.append(\" \".join(temp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Y8OwVKOquf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bag of Words Model\n",
        "#  - rows = sentences\n",
        "#  - columns = words\n",
        "\n",
        "cleaned = df_decreased['text']\n",
        "from sklearn.feature_extraction.text import CountVectorizer #Count of words in each sentence\n",
        "cv = CountVectorizer(max_features=5000, stop_words='english')\n",
        "X = cv.fit_transform(cleaned).toarray()\n",
        "\n",
        "#Bag of words is not good as tfidf because it has no semantic meaning but perform well in sentiment analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld5e3RbGrIkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Term Frequency(tf) = No. of repeated word in sentence / No. of words in sentence\n",
        "Inverse Document Frequency(idf) = log(No. of sentences/No. of sentence containing words)\n",
        "tfidf = tf*idf\n",
        "'''\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "X_tfidf = tfidf.fit_transform(cleaned).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uonbPZGxsef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "f730b2dc-95a4-4734-f4a8-0a31d2bb9706"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df_decreased.label, test_size=0.2, random_state=42)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_pred, y_test))\n",
        "print(accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_pred, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4263  701]\n",
            " [ 754 4282]]\n",
            "0.8545\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85      4964\n",
            "           1       0.86      0.85      0.85      5036\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2KcBGJqg_Sz",
        "colab_type": "text"
      },
      "source": [
        "<h1>*******Using Word2Vec*******<h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgSaatgoPd5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "7f9d1813-e2ac-485a-ef65-216907a1ebcc"
      },
      "source": [
        "######## Word2Vec ##########\n",
        "'''\n",
        "In both bag of words and tfidf, semantic information  is not stored. \n",
        "TF-IDF gives importance to uncommon words.\n",
        "Chance of over fitting.\n",
        "\n",
        "- The solution is Word2Vec. Here, each word is represented as a vector of 32 or\n",
        "more dimension instead of a single number.\n",
        "- Here the semantic information and relation between different words is also\n",
        "preserved\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nIn both bag of words and tfidf, semantic information  is not stored. \\nTF-IDF gives importance to uncommon words.\\nChance of over fitting.\\n\\n- The solution is Word2Vec. Here, each word is represented as a vector of 32 or\\nmore dimension instead of a single number.\\n- Here the semantic information and relation between different words is also\\npreserved\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moltbqvoZk0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "paragraph = \"My 123 name is Rewan Gautam Rewan. I come from the heart of purbanchal, Itahari. It is biggest city of Sunsari district. I am the person you will not predict even with neural networks.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBZwAlW8cbcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data proprocessing\n",
        "\n",
        "text = re.sub(r'\\[[0-9]*\\]', ' ', paragraph)\n",
        "text = re.sub(r'\\s+', ' ', text)\n",
        "text = text.lower()\n",
        "text = re.sub(r'\\d', ' ', text)\n",
        "text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPALHcwPiVMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "396673e0-0d3c-4e52-e829-6a0598ba6426"
      },
      "source": [
        "#Training Word2Vec model\n",
        "\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "words = model.wv.vocab\n",
        "vector = model.wv['rewan']\n",
        "similar = model.wv.most_similar('rewan')\n",
        "print(similar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 0.025892771780490875), ('town', 0.016688115894794464), ('kalabanjar', -0.005918759852647781), ('district', -0.010986454784870148), ('engineer', -0.032506052404642105), ('gautam', -0.042173877358436584), ('born', -0.0720982477068901), ('name', -0.08349581807851791), ('biggest', -0.09935536980628967), ('sunsari', -0.11096066981554031)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJEFOnILoOdl",
        "colab_type": "text"
      },
      "source": [
        "<h1>Fake news classifier</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU6CHKzmi1BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install -q kaggle\n",
        "# from google.colab import files\n",
        "# files.upload()\n",
        "# ! mkdir ~/.kaggle \n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# ! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle datasets list\n",
        "# ! kaggle competitions download -c fake-news\n",
        "# !unzip train.csv.zip\n",
        "# !unzip test.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulh_72m9jKKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7Kd6Qxaqa75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a7fc7249-1c1d-4e23-840a-b76d1307de8e"
      },
      "source": [
        "df = pd.read_csv('/content/train.csv')\n",
        "print(df.columns)\n",
        "df = df.dropna()\n",
        "df.drop('id', axis=1, inplace=True)\n",
        "df['text_new'] = df['title']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['id', 'title', 'author', 'text', 'label'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPJTNtwJqfXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "3c4e4dd2-7875-486d-8adb-5dba340fc6e0"
      },
      "source": [
        "cleaned = df['text_new']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer #Count of words in each sentence\n",
        "cv = CountVectorizer(max_features=5000, stop_words='english')\n",
        "X = cv.fit_transform(cleaned).toarray()\n",
        "\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "# X_tfidf = tfidf.fit_transform(cleaned).toarray()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df.label, test_size=0.2, random_state=42)\n",
        "#print(X_test.shape)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_pred, y_test))\n",
        "print(accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1850  172]\n",
            " [ 232 1403]]\n",
            "0.8895269346458846\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.91      0.90      2022\n",
            "           1       0.89      0.86      0.87      1635\n",
            "\n",
            "    accuracy                           0.89      3657\n",
            "   macro avg       0.89      0.89      0.89      3657\n",
            "weighted avg       0.89      0.89      0.89      3657\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLoP2h840aIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqWqBB-qpyKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddJ9djZn58g_",
        "colab_type": "text"
      },
      "source": [
        "<h1>Fake News Classifier using deep learning with LSTM</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzH0P46XqUtq",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "outputId": "6f5cd183-79a9-4f80-8bce-d3a739f78a07"
      },
      "source": [
        "# ! pip install -q kaggle\n",
        "# from google.colab import files\n",
        "# files.upload()\n",
        "# ! mkdir ~/.kaggle \n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# ! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle datasets list\n",
        "# ! kaggle competitions download -c fake-news\n",
        "# !unzip train.csv.zip\n",
        "# !unzip test.csv.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-14a45a43-c54a-40d6-9c01-9137076ea8f3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-14a45a43-c54a-40d6-9c01-9137076ea8f3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                                               title                                             size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  \n",
            "Cornell-University/arxiv                                          arXiv Dataset                                      2GB  2020-05-06 23:18:35            359  \n",
            "jeffreybraun/chipotle-locations                                   Chipotle Locations                               124KB  2020-07-28 20:20:41            140  \n",
            "christianlillelund/passenger-list-for-the-estonia-ferry-disaster  MS Estonia Disaster Passenger List                14KB  2020-07-26 15:40:17             67  \n",
            "jeffreybraun/chopped-10-years-of-episode-data                     Chopped: 10+ Years of Episode Data               305KB  2020-07-30 15:19:41             37  \n",
            "ahsen1330/us-police-shootings                                     US Police Shootings                              126KB  2020-07-30 04:23:34            274  \n",
            "shuyangli94/this-american-life-podcast-transcriptsalignments      This American Life Podcast Dialog Transcripts     76MB  2020-08-01 20:45:13             12  \n",
            "susuwatari/ppp-loan-data-paycheck-protection-program              PPP Loan Data (Paycheck Protection Program)       26MB  2020-08-01 23:29:41             70  \n",
            "umairnasir14/impact-factor-of-top-1000-journals                   Impact factor of top 1000 journals               397KB  2020-08-03 08:25:35             37  \n",
            "gpreda/covid19-tweets                                             COVID19 Tweets                                    12MB  2020-08-04 08:15:44            181  \n",
            "vidyapb/indian-school-education-statistics                        Indian School Education Statistics                24KB  2020-07-23 17:26:11            243  \n",
            "claytonmiller/ashrae-global-thermal-comfort-database-ii           ASHRAE Global Thermal Comfort Database II          3MB  2020-07-21 02:47:43             51  \n",
            "futurecorporation/epitope-prediction                              COVID-19/SARS B-cell Epitope Prediction            1MB  2020-07-24 02:53:28            138  \n",
            "benroshan/factors-affecting-campus-placement                      Campus Recruitment                                 5KB  2020-04-11 11:09:02          13127  \n",
            "bobbyscience/league-of-legends-diamond-ranked-games-10-min        League of Legends Diamond Ranked Games (10 min)  539KB  2020-04-13 13:53:02           5244  \n",
            "fireballbyedimyrnmom/us-counties-covid-19-dataset                 US counties COVID 19 dataset                       4MB  2020-08-05 15:41:49          11132  \n",
            "divyansh22/flight-delay-prediction                                January Flight Delay Prediction                   23MB  2020-04-14 13:15:41           4401  \n",
            "clmentbisaillon/fake-and-real-news-dataset                        Fake and real news dataset                        41MB  2020-03-26 18:51:15          11174  \n",
            "ikiulian/global-hospital-beds-capacity-for-covid19                Global Hospital Beds Capacity (for covid-19)     284KB  2020-04-26 09:39:35           3803  \n",
            "praveengovi/coronahack-chest-xraydataset                          CoronaHack -Chest X-Ray-Dataset                    1GB  2020-03-20 01:26:40           5288  \n",
            "bappekim/air-pollution-in-seoul                                   Air Pollution in Seoul                            20MB  2020-04-03 16:33:49           5325  \n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            " 73% 27.0M/37.0M [00:01<00:01, 9.56MB/s]\n",
            "100% 37.0M/37.0M [00:01<00:00, 20.2MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 53% 5.00M/9.42M [00:01<00:00, 5.19MB/s]\n",
            "100% 9.42M/9.42M [00:01<00:00, 9.24MB/s]\n",
            "Downloading submit.csv to /content\n",
            "  0% 0.00/40.6k [00:00<?, ?B/s]\n",
            "100% 40.6k/40.6k [00:00<00:00, 46.8MB/s]\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8y-op_iC2Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOBeU9lVDRti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/train.csv')\n",
        "df = df.dropna()\n",
        "X = df.drop('label', axis=1)\n",
        "y = df['label']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDIsGZxIDZYw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8ee77312-e225-4c6e-f404-e1da463271da"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_0z5NWKEOgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing\n",
        "\n",
        "text = X.copy()\n",
        "text.reset_index(inplace=True)\n",
        "\n",
        "corpus = []\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for i in range(0, len(text)):\n",
        "  cleaning = re.sub('[^a-zA-Z]', ' ', text['title'][i])\n",
        "  cleaning = cleaning.lower()\n",
        "  cleaning = cleaning.split()\n",
        "\n",
        "  cleaning = [ps.stem(word) for word in cleaning if not word in stopwords.words('english')]\n",
        "  cleaning = \" \".join(cleaning)\n",
        "  corpus.append(cleaning)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLejYV4gMvk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc_size = 5000 #Number of words for the one hot encoding\n",
        "sent_length = 20 #Max length for padding\n",
        "embedding_vector_features = 40 #Number of vector features for embedding\n",
        "\n",
        "#One hot encoding\n",
        "onehot_repr = [one_hot(sentence, voc_size) for sentence in corpus]\n",
        "\n",
        "#Padding\n",
        "embedded_docs = pad_sequences(onehot_repr, padding='pre', maxlen=sent_length)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcLQ58kPpof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "60bc6082-83bc-41c2-c7eb-606ec67dac76"
      },
      "source": [
        "#Model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(LSTM(100))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 20, 40)            200000    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 20, 40)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 20, 100)           56400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 20, 100)           400       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 20, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 20, 100)           80400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 20, 100)           400       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 20, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 418,501\n",
            "Trainable params: 417,901\n",
            "Non-trainable params: 600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlkxaXjYR-NC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting to numpy array\n",
        "\n",
        "X_final = np.array(embedded_docs)\n",
        "\n",
        "y_final = np.array(y)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhVEDL-3TOOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "374b7b4a-13dd-4fa1-b177-c999ffc6b10f"
      },
      "source": [
        "#Splitting dataset to training and testing \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=77)\n",
        "\n",
        "#Model training\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=100)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "147/147 [==============================] - 23s 153ms/step - loss: 0.2793 - accuracy: 0.8820 - val_loss: 0.5827 - val_accuracy: 0.5649\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 21s 142ms/step - loss: 0.1584 - accuracy: 0.9380 - val_loss: 0.8199 - val_accuracy: 0.5346\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 21s 145ms/step - loss: 0.1211 - accuracy: 0.9532 - val_loss: 0.2060 - val_accuracy: 0.9196\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 21s 144ms/step - loss: 0.1037 - accuracy: 0.9624 - val_loss: 0.2149 - val_accuracy: 0.9141\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 21s 145ms/step - loss: 0.0896 - accuracy: 0.9677 - val_loss: 0.2902 - val_accuracy: 0.9128\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 21s 144ms/step - loss: 0.0721 - accuracy: 0.9739 - val_loss: 0.3156 - val_accuracy: 0.9196\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 21s 146ms/step - loss: 0.0689 - accuracy: 0.9758 - val_loss: 0.3427 - val_accuracy: 0.9103\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 21s 146ms/step - loss: 0.0546 - accuracy: 0.9825 - val_loss: 0.3028 - val_accuracy: 0.9193\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 22s 146ms/step - loss: 0.0498 - accuracy: 0.9841 - val_loss: 0.2965 - val_accuracy: 0.9130\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 21s 145ms/step - loss: 0.0516 - accuracy: 0.9820 - val_loss: 0.3511 - val_accuracy: 0.9174\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 21s 145ms/step - loss: 0.0476 - accuracy: 0.9852 - val_loss: 0.3338 - val_accuracy: 0.9199\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 21s 145ms/step - loss: 0.0548 - accuracy: 0.9816 - val_loss: 0.3693 - val_accuracy: 0.8964\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 21s 144ms/step - loss: 0.0439 - accuracy: 0.9849 - val_loss: 0.3959 - val_accuracy: 0.9185\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 21s 146ms/step - loss: 0.0407 - accuracy: 0.9867 - val_loss: 0.3694 - val_accuracy: 0.9111\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 22s 147ms/step - loss: 0.0419 - accuracy: 0.9852 - val_loss: 0.4720 - val_accuracy: 0.9152\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 21s 146ms/step - loss: 0.0473 - accuracy: 0.9849 - val_loss: 0.4709 - val_accuracy: 0.9136\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 21s 146ms/step - loss: 0.0508 - accuracy: 0.9857 - val_loss: 0.4319 - val_accuracy: 0.9106\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 21s 145ms/step - loss: 0.0496 - accuracy: 0.9844 - val_loss: 0.3218 - val_accuracy: 0.9144\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 21s 146ms/step - loss: 0.0490 - accuracy: 0.9833 - val_loss: 0.3501 - val_accuracy: 0.9139\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 22s 147ms/step - loss: 0.0479 - accuracy: 0.9854 - val_loss: 0.3076 - val_accuracy: 0.9100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f721dbf74e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEp2zoBkUjli",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "862f13d2-4d4d-41db-ae61-241036fa20b9"
      },
      "source": [
        "#Model performance\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_pred = model.predict_classes(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1886  180]\n",
            " [ 149 1442]]\n",
            "0.910035548263604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XanzCrHlVaah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "6fe74d72-5ad3-4d31-f663-a0f44f47b0c7"
      },
      "source": [
        "def preprocessing_and_all(dataframe_dir):\n",
        "  df_test = pd.read_csv(dataframe_dir)\n",
        "  df_temp = df_test.copy()\n",
        "  df_temp['text'] = df_temp['text'].str[:10]\n",
        "  df_temp['title'] = df_temp['title'].fillna(df_temp['text'])\n",
        "  df_temp.reset_index(inplace=True)\n",
        "\n",
        "  corpus = []\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  for i in range(0, len(df_temp)):\n",
        "    cleaning = re.sub('[^a-zA-Z]', ' ', df_temp['title'][i])\n",
        "    cleaning = cleaning.lower()\n",
        "    cleaning = cleaning.split()\n",
        "\n",
        "    cleaning = [ps.stem(word) for word in cleaning if not word in stopwords.words('english')]\n",
        "    cleaning = \" \".join(cleaning)\n",
        "    corpus.append(cleaning)\n",
        "  \n",
        "  voc_size = 5000 #Number of words for the one hot encoding\n",
        "  sent_length = 20 #Max length for padding\n",
        "  embedding_vector_features = 40 #Number of vector features for embedding\n",
        "\n",
        "  #One hot encoding\n",
        "  onehot_repr = [one_hot(sentence, voc_size) for sentence in corpus]\n",
        "\n",
        "  #Padding\n",
        "  embedded_docs = pad_sequences(onehot_repr, padding='pre', maxlen=sent_length)\n",
        "  \n",
        "  X = np.array(embedded_docs)\n",
        "  y_pred_real = model.predict_classes(X)  \n",
        "  df_temp['label'] = y_pred_real\n",
        "  df_to_submit = df_temp[['id', 'label']]\n",
        "  return df_to_submit\n",
        "\n",
        "df_sum = preprocessing_and_all('/content/test.csv')\n",
        "print(df_sum.head())\n",
        "print(df_sum.info())\n",
        "df_sum.to_csv('/content/final.csv', index=False)\n",
        "!kaggle competitions submit fake-news -f /content/final.csv -m \"My submission message\""
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id  label\n",
            "0  20800      0\n",
            "1  20801      1\n",
            "2  20802      0\n",
            "3  20803      0\n",
            "4  20804      1\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5200 entries, 0 to 5199\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype\n",
            "---  ------  --------------  -----\n",
            " 0   id      5200 non-null   int64\n",
            " 1   label   5200 non-null   int32\n",
            "dtypes: int32(1), int64(1)\n",
            "memory usage: 61.1 KB\n",
            "None\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 40.6k/40.6k [00:06<00:00, 6.32kB/s]\n",
            "Successfully submitted to Fake News"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmNIVCjsuN1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFNN3w-wwgsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgQI5lKxwxr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGMSb3qow2PW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9-dyNxIxNLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}